{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNFUZIdCO9vZl1BlQ3Aj7bh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/2303A51889/AIMI--2025/blob/main/01_2303A51889_Lab_Exam_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Frediction of startup opportunities in Turkish Business**\n",
        "1. Identify the Top-10 Startups in turkey.\n",
        "2. Find the max and Min crowd sourced funding obtained by turkey.\n",
        "3. Name the gender which has acquired the most funding for startups.\n",
        "4. Identify the top-5 cities where startup is most.\n",
        "5. Name the top-5 technologies popular in Turkish startups.\n",
        "6. Apply either Classification or Clustering or regression model  and evaluate the accuracy, error metrics of the model."
      ],
      "metadata": {
        "id": "WNGlNKC1CHwk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report"
      ],
      "metadata": {
        "id": "aWfwPlz7QzPg"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/turkishCF"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "caRvAg4eQ5cr",
        "outputId": "17b52ef7-96dd-48c1-9cb5-b6beb61f37aa"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "unterminated string literal (detected at line 1) (<ipython-input-69-19a899d1cb99>, line 1)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-69-19a899d1cb99>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    df = pd.read_csv('/content/turkishCF\u001b[0m\n\u001b[0m                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated string literal (detected at line 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Load the dataset (adjust path as necessary)\n",
        "df = pd.read_csv('/content/turkishCF.csv')  # Updated with the correct file path\n",
        "\n",
        "\n",
        "# Step 1: Identify the Top-10 Startups based on funding\n",
        "print(\"Columns in the dataset:\", df.columns)  # Check column names to confirm\n",
        "\n",
        "# Ensure 'Startup Name' and 'Funding' columns exist\n",
        "if 'Startup Name' in df.columns and 'Funding' in df.columns:\n",
        "    # Sort by 'Funding' in descending order to get top 10 startups\n",
        "    top_10_startups = df.sort_values(by='Funding', ascending=False).head(10)\n",
        "    print(\"\\nTop 10 Startups in Turkey based on Funding:\")\n",
        "    print(top_10_startups[['Startup Name', 'Funding']])\n",
        "else:\n",
        "    print(\"\\nError: The columns 'Startup Name' or 'Funding' do not exist. Please check the column names.\")\n",
        "\n",
        "# Step 2: Find the Max and Min Crowdsourced Funding\n",
        "if 'Funding' in df.columns:\n",
        "    # Convert 'Funding' to numeric to handle non-numeric values\n",
        "    df['Funding'] = pd.to_numeric(df['Funding'], errors='coerce')\n",
        "\n",
        "    # Drop rows with missing 'Funding'\n",
        "    df = df.dropna(subset=['Funding'])\n",
        "\n",
        "    # Calculate the max and min funding\n",
        "    max_funding = df['Funding'].max()\n",
        "    min_funding = df['Funding'].min()\n",
        "    print(f\"\\nMaximum Funding: {max_funding}\")\n",
        "    print(f\"Minimum Funding: {min_funding}\")\n",
        "else:\n",
        "    print(\"\\nError: 'Funding' column not found in the dataset. Please check the column names.\")\n",
        "\n",
        "# Step 3: Identify Gender with Most Funding\n",
        "if 'founder_gender' in df.columns and 'Funding' in df.columns:\n",
        "    # Convert 'Funding' to numeric to handle non-numeric values\n",
        "    df['Funding'] = pd.to_numeric(df['Funding'], errors='coerce')\n",
        "\n",
        "    # Drop rows with missing 'founder_gender' or 'Funding'\n",
        "    df = df.dropna(subset=['founder_gender', 'Funding'])\n",
        "\n",
        "    # Group by 'founder_gender' and sum the funding\n",
        "    gender_funding = df.groupby('founder_gender')['Funding'].sum()\n",
        "\n",
        "    if not gender_funding.empty:\n",
        "        most_funded_gender = gender_funding.idxmax()\n",
        "        print(f\"\\nGender with Most Funding: {most_funded_gender}\")\n",
        "    else:\n",
        "        print(\"\\nNo valid data available to determine the gender with most funding.\")\n",
        "else:\n",
        "    print(\"\\nError: The columns 'founder_gender' or 'Funding' are missing. Please check the column names.\")\n",
        "\n",
        "# Step 4: Identify Top-5 Cities with Most Startups\n",
        "if 'city' in df.columns:\n",
        "    top_cities = df['city'].value_counts().head(5)\n",
        "    print(\"\\nTop 5 Cities with Most Startups:\")\n",
        "    print(top_cities)\n",
        "else:\n",
        "    print(\"\\nError: 'city' column not found in the dataset. Please check the column names.\")\n",
        "\n",
        "# Step 5: Identify Top-5 Technologies in Turkish Startups\n",
        "if 'technology_sector' in df.columns:\n",
        "    top_technologies = df['technology_sector'].value_counts().head(5)\n",
        "    print(\"\\nTop 5 Technologies in Turkish Startups:\")\n",
        "    print(top_technologies)\n",
        "else:\n",
        "    print(\"\\nError: 'technology_sector' column not found in the dataset. Please check the column names.\")\n",
        "\n",
        "# Step 6: Build and Evaluate a Classification Model\n",
        "# Ensure necessary columns are present\n",
        "if 'success' in df.columns and 'Funding' in df.columns and 'city' in df.columns and 'founder_gender' in df.columns and 'technology_sector' in df.columns:\n",
        "    # Encode categorical variables\n",
        "    label_encoders = {}\n",
        "    for column in ['city', 'founder_gender', 'technology_sector']:\n",
        "        le = LabelEncoder()\n",
        "        df[column] = le.fit_transform(df[column])\n",
        "        label_encoders[column] = le\n",
        "\n",
        "    # Prepare feature matrix and target\n",
        "    X = df[['Funding', 'city', 'founder_gender', 'technology_sector']]\n",
        "    y = df['success']\n",
        "\n",
        "    # Standardize the 'Funding' column\n",
        "    scaler = StandardScaler()\n",
        "    X['Funding'] = scaler.fit_transform(X[['Funding']])\n",
        "\n",
        "    # Train-test split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Train logistic regression model\n",
        "    model = LogisticRegression()\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Predict and evaluate\n",
        "    y_pred = model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(f\"\\nModel Accuracy: {accuracy:.2f}\")\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "else:\n",
        "    print(\"\\nError: Missing necessary columns for the model. Please ensure 'success', 'Funding', 'city', 'founder_gender', and 'technology_sector' are present.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "HZco4l18QIXr",
        "outputId": "ed3c415f-3994-4fd0-bd8e-32af26f6e832"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ParserError",
          "evalue": "Error tokenizing data. C error: Expected 1 fields in line 3, saw 2\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-61-bb4bc817fe39>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Load the dataset (adjust path as necessary)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/turkishCF.csv'\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Updated with the correct file path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    624\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 626\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1921\u001b[0m                     \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1922\u001b[0m                     \u001b[0mcol_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1923\u001b[0;31m                 \u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1924\u001b[0m                     \u001b[0mnrows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1925\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlow_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m                 \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_low_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m                 \u001b[0;31m# destructive to chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_concatenate_chunks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 1 fields in line 3, saw 2\n"
          ]
        }
      ]
    }
  ]
}